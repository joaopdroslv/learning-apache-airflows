# üîç **O que √© o Apache Airflow?**

O **Apache Airflow** √© uma plataforma de orquestra√ß√£o de workflows que permite definir, agendar e monitorar fluxos de trabalho. 

Ele foi projetado para tarefas de ETL, pipelines de machine learning, automa√ß√£o de processos de dados e execu√ß√£o de jobs distribu√≠dos.

A ideia principal do Airflow √© representar um **fluxo de trabalho como um DAG (Directed Acyclic Graph)**.

# üìå **Conceitos Fundamentais do Airflow**

## 1Ô∏è. **DAG (Directed Acyclic Graph)**
Uma **DAG** define um fluxo de trabalho e suas depend√™ncias. Ela cont√©m **tarefas (Tasks)**, que s√£o organizadas de forma a n√£o criar ciclos (loops infinitos).

üí° **Por que DAGs e n√£o simplesmente scripts sequenciais?**

- DAGs permitem **orquestra√ß√£o** e n√£o apenas execu√ß√£o linear.
- O Airflow **rastrea estado de execu√ß√£o** e **reexecuta tarefas** conforme necess√°rio.
- A DAG √© **declarativa**: voc√™ define as tarefas e suas depend√™ncias, e o Airflow agenda e executa corretamente.

**Estrutura de uma DAG**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def tarefa_exemplo():
    print("Executando minha primeira DAG!")

with DAG(
    dag_id="minha_primeira_dag",
    start_date=datetime(2024, 1, 1),
    schedule_interval="0 12 * * *",  # Executa diariamente ao meio-dia
    catchup=False,
) as dag:
    
    tarefa = PythonOperator(
        task_id="executar_script",
        python_callable=tarefa_exemplo,
    )

tarefa
```

**Analisando a DAG acima**
- `dag_id`: Nome da DAG, usado internamente no Airflow.
- `start_date`: Data de in√≠cio da DAG. O Airflow n√£o executa DAGs antes desta data.
- `schedule_interval`: Intervalo de execu√ß√£o (formato cron).
- `catchup`: Se False, DAGs atrasadas n√£o s√£o executadas retroativamente.
- `PythonOperator`: Um tipo de tarefa que executa fun√ß√µes Python.

## 2. **Operators & Tasks**

Os **operators** s√£o modelos predefinidos de tarefas. Eles s√£o a base das tarefas executadas dentro das DAGs.

üìå Tipos principais de Operators:
‚úÖ `PythonOperator` ‚Üí Executa uma fun√ß√£o Python.
‚úÖ `BashOperator` ‚Üí Executa comandos no shell.
‚úÖ `PostgresOperator` ‚Üí Executa queries SQL em bancos de dados PostgreSQL.
‚úÖ `HttpOperator` ‚Üí Faz requisi√ß√µes HTTP para APIs externas.
‚úÖ `DummyOperator` ‚Üí Cria tarefas vazias (√∫teis para organiza√ß√£o).

**Exemplo de DAG com m√∫ltiplos operadores:**
```python
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

def processamento():
    print("Processando dados...")

tarefa1 = BashOperator(
    task_id="criar_arquivo",
    bash_command="echo 'Arquivo criado' > /tmp/arquivo.txt"
)

tarefa2 = PythonOperator(
    task_id="processar_dados",
    python_callable=processamento
)

tarefa1 >> tarefa2  # Defini√ß√£o da ordem de execu√ß√£o
```

üõ†Ô∏è **Observa√ß√£o:**
Cada tarefa dentro de uma DAG pode **rodar em paralelo**, dependendo da configura√ß√£o do executor e do agendador.

## 3. **Scheduler & Executor**

O **scheduler** (agendador) verifica quais DAGs precisam ser executadas com base no hor√°rio agendado e dispara as tarefas.

O **executor** gerencia a execu√ß√£o das tarefas e pode distribu√≠-las entre m√∫ltiplos workers, dependendo da configura√ß√£o.

### Tipos de Executors
1. `SequentialExecutor` ‚Üí Apenas uma tarefa por vez (limitado, mas √∫til para testes).
2. `LocalExecutor` ‚Üí Execu√ß√£o paralela em um √∫nico servidor.
3. `CeleryExecutor` ‚Üí Distribui tarefas entre m√∫ltiplos workers.
4. `KubernetesExecutor` ‚Üí Cria pods Kubernetes para execu√ß√£o din√¢mica de tarefas.

**üí° Por que usar o CeleryExecutor?**
- Permite **escalar** a execu√ß√£o para m√∫ltiplos servidores.
- As tarefas podem ser distribu√≠das em uma fila de mensagens (**Redis**, **RabbitMQ**).

```ini
[core]
executor = CeleryExecutor

[celery]
broker_url = redis://localhost:6379/0
result_backend = db+postgresql://airflow:password@localhost/airflow
```

## 4. **Defini√ß√£o de Depend√™ncias**

O Airflow permite definir depend√™ncias entre tarefas para criar workflows sofisticados.

**Exemplo de depend√™ncias simples**
```python
task1 >> task2  # task2 s√≥ executa ap√≥s task1
task2 << task3  # task3 deve rodar antes de task2
task3 >> [task4, task5]  # task4 e task5 executam ap√≥s task3
```

## 5. **Execu√ß√£o e Monitoramento**

**Passos para rodar o Airflow**

1. **Iniciar o banco de dados interno**
```sh
airflow db init
```

2. **Criar um usu√°rio administrador**
```sh
airflow users create --username admin --password admin --role Admin --email admin@example.com
```

3. **Rodar o scheduler (agendador)**
```sh
airflow scheduler
```

4. **Rodar o servidor web**
```sh
airflow webserver --port 8080
```

**Acessar a interface** ‚Üí http://localhost:8080
